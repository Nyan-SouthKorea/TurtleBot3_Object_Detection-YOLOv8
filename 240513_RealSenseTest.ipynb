{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "관련 Github, 링크\n",
    "github: https://github.com/IntelRealSense/librealsense/blob/master/wrappers/python/examples/align-depth2color.py\n",
    "공식 Docs: https://dev.intelrealsense.com/docs/tensorflow-with-intel-realsense-cameras\n",
    "'''\n",
    "# doc의 공식 코드\n",
    "\n",
    "\n",
    "# depth color map이랑 rgb카메라 스트리밍 공식 코드\n",
    "## License: Apache 2.0. See LICENSE file in root directory.\n",
    "## Copyright(c) 2015-2017 Intel Corporation. All Rights Reserved.\n",
    "\n",
    "###############################################\n",
    "##      Open CV and Numpy integration        ##\n",
    "###############################################\n",
    "\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Configure depth and color streams\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "\n",
    "# Get device product line for setting a supporting resolution\n",
    "pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
    "pipeline_profile = config.resolve(pipeline_wrapper)\n",
    "device = pipeline_profile.get_device()\n",
    "device_product_line = str(device.get_info(rs.camera_info.product_line))\n",
    "\n",
    "found_rgb = False\n",
    "for s in device.sensors:\n",
    "    if s.get_info(rs.camera_info.name) == 'RGB Camera':\n",
    "        found_rgb = True\n",
    "        break\n",
    "if not found_rgb:\n",
    "    print(\"The demo requires Depth camera with Color sensor\")\n",
    "    exit(0)\n",
    "\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "\n",
    "# Start streaming\n",
    "pipeline.start(config)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Wait for a coherent pair of frames: depth and color\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert images to numpy arrays\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "\n",
    "        depth_colormap_dim = depth_colormap.shape\n",
    "        color_colormap_dim = color_image.shape\n",
    "\n",
    "        # If depth and color resolutions are different, resize color image to match depth image for display\n",
    "        if depth_colormap_dim != color_colormap_dim:\n",
    "            resized_color_image = cv2.resize(color_image, dsize=(depth_colormap_dim[1], depth_colormap_dim[0]), interpolation=cv2.INTER_AREA)\n",
    "            images = np.hstack((resized_color_image, depth_colormap))\n",
    "        else:\n",
    "            images = np.hstack((color_image, depth_colormap))\n",
    "\n",
    "        # Show images\n",
    "        cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "        cv2.imshow('RealSense', images)\n",
    "        cv2.imshow('Depth Image', depth_image) # 라이언 추가\n",
    "        if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    # Stop streaming\n",
    "    pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 240514 라이언 제작 모듈\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "class real_sense:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Real Sense 카메라 연결하여 파이썬으로 RGB 이미지와 Depth Map을 불러오는 Class.\n",
    "        공식 문서를 참고하여 사용하기 쉽게 Class의 형태로 제작함\n",
    "        '''\n",
    "        # Configure depth and color streams\n",
    "        self.pipeline = rs.pipeline()\n",
    "        config = rs.config()\n",
    "\n",
    "        # Get device product line for setting a supporting resolution\n",
    "        pipeline_wrapper = rs.pipeline_wrapper(self.pipeline)\n",
    "        pipeline_profile = config.resolve(pipeline_wrapper)\n",
    "        device = pipeline_profile.get_device()\n",
    "        device_product_line = str(device.get_info(rs.camera_info.product_line))\n",
    "\n",
    "        found_rgb = False\n",
    "        for s in device.sensors:\n",
    "            if s.get_info(rs.camera_info.name) == 'RGB Camera':\n",
    "                found_rgb = True\n",
    "                break\n",
    "        if not found_rgb:\n",
    "            print(\"The demo requires Depth camera with Color sensor\")\n",
    "            exit(0)\n",
    "\n",
    "        config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "        config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "\n",
    "        # Start streaming\n",
    "        self.pipeline.start(config)\n",
    "        print('Real Sense 카메라 수신 설정 완료')\n",
    "\n",
    "    def get_cam(self):\n",
    "        '''\n",
    "        RealSense 카메라에서 frame들을 가져옴.\n",
    "        이 frame들에서 RGB이미지, Depth맵 등 다양한 이미지가 추출됨\n",
    "        frame이 return되지는 않고, 내부 변수로 활용됨\n",
    "        '''\n",
    "        self.frames = self.pipeline.wait_for_frames()\n",
    "    \n",
    "    def get_color_img(self):\n",
    "        '''\n",
    "        get_cam 실행 이후에 실행되어야 에러가 발생하지 않음\n",
    "        self.frames라는 변수가 존재해야 실행 가능한 로직이기 때문임\n",
    "        '''\n",
    "        color_frame = self.frames.get_color_frame()\n",
    "        if color_frame:\n",
    "            self.color_image = np.asanyarray(color_frame.get_data())\n",
    "            return self.color_image\n",
    "        else:\n",
    "            print('get_color_img 에러 발생!')\n",
    "        \n",
    "    def get_depth_img(self):\n",
    "        '''\n",
    "        get_cam 실행 이후에 실행되어야 에러가 발생하지 않음\n",
    "        self.frames라는 변수가 존재해야 실행 가능한 로직이기 때문임\n",
    "        '''\n",
    "        depth_frame = self.frames.get_depth_frame()\n",
    "        if depth_frame:\n",
    "            self.depth_image = np.asanyarray(depth_frame.get_data())\n",
    "            return self.depth_image\n",
    "        else:\n",
    "            print('get_depth_img 에러 발생!')\n",
    "\n",
    "    def get_depth_color_map(self):\n",
    "        '''\n",
    "        self.depth_image로부터 color_map을 만들어서 반환함\n",
    "        반드시 get_depth_img 함수가 실행되어야 에러가 발생하지 않음\n",
    "        self.depth_image가 존재해야 실행 가능한 로직이기 때문임\n",
    "        '''\n",
    "        self.depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(self.depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "        return self.depth_colormap\n",
    "    \n",
    "    def concat_all(self):\n",
    "        '''\n",
    "        위에서 만든 color_img, depth_map, depth_color_map 3개를 가로 Concat하여 반환\n",
    "        위 3개 이미지가 모두 존재해야 에러가 발생하지 않음\n",
    "        위 3개 함수를 선언하지 않고 코드를 실행하면 이전 frame의 이미지가 concat될 것임(주의 필요)\n",
    "        '''\n",
    "        # 깊이 이미지의 최대값을 확인하고, 8비트 형식으로 스케일 조정\n",
    "        scaled_depth_image = cv2.convertScaleAbs(self.depth_image, alpha=(255.0/np.max(self.depth_image)))\n",
    "        depth_3d = cv2.cvtColor(scaled_depth_image, cv2.COLOR_GRAY2BGR) # 1차원인 depth 이미지를 3차원으로 변경함\n",
    "        concatenated_image = cv2.hconcat([self.color_image, depth_3d, self.depth_colormap])\n",
    "        return concatenated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Sense 카메라 수신 설정 완료\n"
     ]
    }
   ],
   "source": [
    "RealSense = real_sense()\n",
    "while True:\n",
    "    RealSense.get_cam() # 카메라 수신\n",
    "    color_img = RealSense.get_color_img()\n",
    "    depth_img = RealSense.get_depth_img()\n",
    "    depth_color_map = RealSense.get_depth_color_map()\n",
    "    cv2.imshow('Real Sense Streaming[color_img, depth_img, depth_color_map]', RealSense.concat_all())\n",
    "    cv2.imshow('depth map', RealSense.depth_image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "# YOLOv8 연동\n",
    "class YOLOv8:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        YOLOv8모델 로드 및 인퍼런스 준비\n",
    "        '''\n",
    "        self.model = YOLO('./yolov8n.pt')\n",
    "        print('YOLOv8 모델 로드 완료')\n",
    "        self.cls_list = 'none'\n",
    "    \n",
    "    def run(self, img, depth_map):\n",
    "        '''\n",
    "        img: YOLOv8로 추론할 이미지 입력\n",
    "        추론 결과인 dic_list 반환(bbox[x1, y1, x2, y2], conf, class_name 포함)\n",
    "        depth_map도 같이 받아서 물체의 거리 반환\n",
    "        '''\n",
    "        self.img = img\n",
    "        results = self.model.predict(source = cv2.cvtColor(img, cv2.COLOR_BGR2RGB), verbose = False)[0]\n",
    "        if self.cls_list == 'none':\n",
    "            cls_list = results.names\n",
    "        results = results.boxes.data.tolist()\n",
    "        self.dic_list = []\n",
    "        for result in results:\n",
    "            x1, y1, x2, y2, conf, cls = int(result[0]), int(result[1]), int(result[2]), int(result[3]), float(result[4]), int(result[5])\n",
    "            # depth 추출\n",
    "            small_x1, small_y1, small_x2, small_y2 = self.make_small_bbox(x1, y1, x2, y2)\n",
    "            depth = self.calculate_average_depth(depth_map, small_x1, small_y1, small_x2, small_y2)\n",
    "            self.dic_list.append({'bbox':[x1, y1, x2, y2], 'conf':conf, 'class_name':cls_list[cls], 'depth':depth})\n",
    "        return self.dic_list\n",
    "    \n",
    "    def draw(self):\n",
    "        '''\n",
    "        추론한 결과를 이미지에 그려서 반환해주는 함수.\n",
    "        내부 변수로 img와 dic_list를 가지고 있기 때문에 인자로 넣어줄 필요 없음.\n",
    "        그려진 이미지 반환. 이 함수를 실행하는 순간 내부 self.img는 그려진 이미지로 바뀜\n",
    "        '''\n",
    "        for dic in self.dic_list:\n",
    "            cv2.rectangle(self.img, (dic['bbox'][0], dic['bbox'][1]), (dic['bbox'][2], dic['bbox'][3]), (0,0,255), 2)\n",
    "            text = f'{dic[\"class_name\"]}:{round(dic[\"conf\"], 2)}'\n",
    "            cv2.putText(self.img, text, (dic['bbox'][0]-5, dic['bbox'][1]-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "        return self.img\n",
    "\n",
    "    def make_small_bbox(x1, y1, x2, y2):\n",
    "        '''\n",
    "        bbox를 넣으면 일정 비율로 축소해주는 함수\n",
    "        '''\n",
    "        ratio = 0.1\n",
    "        center_x, center_y = (x1+x2)/2, (y1+y2)/2\n",
    "        x, y = x2-x1, y2-y1\n",
    "        small_x, small_y = x*ratio, y*ratio\n",
    "        new_x1 = center_x - (small_x/2)\n",
    "        new_y1 = center_y - (small_y/2)\n",
    "        new_x2 = new_x1 + small_x\n",
    "        new_y2 = new_y1 + small_y\n",
    "        return int(new_x1), int(new_y1), int(new_x2), int(new_y2)\n",
    "        \n",
    "\n",
    "    def calculate_average_depth(depth_map, x1, y1, x2, y2):\n",
    "        \"\"\"\n",
    "        지정된 bounding box 내에서 평균 깊이를 계산하는 함수.\n",
    "        \n",
    "        Parameters:\n",
    "        depth_image (numpy array): 깊이 이미지 (2D numpy array 형태).\n",
    "        x1, y1 (int): bounding box의 왼쪽 상단 좌표.\n",
    "        x2, y2 (int): bounding box의 오른쪽 하단 좌표.\n",
    "        \n",
    "        Returns:\n",
    "        float: 계산된 평균 깊이 값. 0을 제외한 깊이 값의 평균.\n",
    "        \"\"\"\n",
    "        # bounding box 영역에서 깊이 데이터를 추출\n",
    "        depth_region = depth_image[y1:y2, x1:x2]\n",
    "        \n",
    "        # 0을 제외한 깊이 값만을 사용하여 평균 계산\n",
    "        if np.count_nonzero(depth_region) > 0:\n",
    "            average_depth = np.mean(depth_region[depth_region > 0])\n",
    "            return average_depth\n",
    "        else:\n",
    "            print(\"해당 영역에 유효한 깊이 데이터가 없습니다.\")\n",
    "            return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Sense 카메라 수신 설정 완료\n",
      "YOLOv8 모델 로드 완료\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "make_small_bbox() takes 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m color_img \u001b[38;5;241m=\u001b[39m RealSense\u001b[38;5;241m.\u001b[39mget_color_img()\n\u001b[0;32m      6\u001b[0m depth_map \u001b[38;5;241m=\u001b[39m RealSense\u001b[38;5;241m.\u001b[39mget_depth_img()\n\u001b[1;32m----> 7\u001b[0m \u001b[43mYolov8\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolor_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m draw_img \u001b[38;5;241m=\u001b[39m Yolov8\u001b[38;5;241m.\u001b[39mdraw()\n\u001b[0;32m      9\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOLOv8 Inference\u001b[39m\u001b[38;5;124m'\u001b[39m, draw_img)\n",
      "Cell \u001b[1;32mIn[75], line 29\u001b[0m, in \u001b[0;36mYOLOv8.run\u001b[1;34m(self, img, depth_map)\u001b[0m\n\u001b[0;32m     27\u001b[0m x1, y1, x2, y2, conf, \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(result[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mint\u001b[39m(result[\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28mint\u001b[39m(result[\u001b[38;5;241m2\u001b[39m]), \u001b[38;5;28mint\u001b[39m(result[\u001b[38;5;241m3\u001b[39m]), \u001b[38;5;28mfloat\u001b[39m(result[\u001b[38;5;241m4\u001b[39m]), \u001b[38;5;28mint\u001b[39m(result[\u001b[38;5;241m5\u001b[39m])\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# depth 추출\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m small_x1, small_y1, small_x2, small_y2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_small_bbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_average_depth(depth_map, small_x1, small_y1, small_x2, small_y2)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdic_list\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m'\u001b[39m:[x1, y1, x2, y2], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconf\u001b[39m\u001b[38;5;124m'\u001b[39m:conf, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m'\u001b[39m:cls_list[\u001b[38;5;28mcls\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m:depth})\n",
      "\u001b[1;31mTypeError\u001b[0m: make_small_bbox() takes 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "RealSense = real_sense()\n",
    "Yolov8 = YOLOv8()\n",
    "while True:\n",
    "    RealSense.get_cam() # 카메라 수신\n",
    "    color_img = RealSense.get_color_img()\n",
    "    depth_map = RealSense.get_depth_img()\n",
    "    Yolov8.run(color_img, depth_map)\n",
    "    draw_img = Yolov8.draw()\n",
    "    cv2.imshow('YOLOv8 Inference', draw_img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
